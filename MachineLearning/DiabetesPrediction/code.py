# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

"""

"""
Mount Google Drive (optional)
"""

'''
from google.colab import drive
drive.mount('/content/drive')

import os
# os.chdir("/content/drive/MyDrive/....")  # file path
print(os.getcwd())
'''

"""
# **HW2 : Decision Tree and Random Forest**
In *assignment 2*, you need to finish :

1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes
> * Step 1 : Load the input data
> * Step 2 : Calculate the Entropy and Information Gain
> * Step 3 : Find the Best Split
> * Step 4 : Split into 2 branches
> * Step 5 : Build decision tree
> * Step 6 : Save the answers from step2 to step5
> * Step 7 : Split data into training set and validation set
> * Step 8 : Train a decision tree model with training set
> * Step 9 : Predict the cases in the *validation set* by using the model trained in *Step8*
> * Step 10 : Calculate the f1-score of your predictions in *Step9*
> * Step 11 : Write the Output File

2. Advanced Part : Build a **Random Forest** model to make predictions
> * Step 1 : Load the input data
> * Step 2 : Load the test data
> * Step 3 : Build a random forest
> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*
> * Step 5 : Save the predictions(from *Step 4*) in a csv file

"""

"""
# **Basic Part** (60%)
In this part, your need to implement a Decision Tree model by completing the following given functions.

Also, you need to run these functions with the given input variables and save the output in a csv file **hw2_basic.csv**

## Import Packages


> Note : You **cannot** import any other packages in both basic part and advanced part
"""

import numpy as np
import pandas as pd
import math
import random
from numpy import sqrt
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

"""
## Step1: Load the input data
First, load the input file **hw2_input_basic.csv**
"""

input_data = pd.read_csv('hw2_input_basic.csv')
input_data

"""
## Global attributes
Define the global attributes
> Note : You **cannot** modify the values of these attributes we given in the basic part
"""

max_depth = 2
depth = 0
min_samples_split = 2
n_features = input_data.shape[1] - 1

"""> You can add your own global attributes here"""

Label = "diabetes_mellitus"

"""
## Step2 : Calculate the Entropy and Information Gain
Calculate the information gain and entropy values before separate data into left subtree and right subtree
"""

def entropy(data):
    """
    This function measures the amount of uncertainty in a probability distribution
    args:
    * data(type: DataFrame): the data you're calculating for the entropy
    return:
    * entropy_value(type: float): the data's entropy
    """
    l = data[Label]
    p = l.value_counts()/l.shape[0]
    entropy_value = np.sum(-p*np.log2(p))
    return entropy_value

# [Note] You have to save the value of "ans_entropy" into the output file
ans_entropy = entropy(input_data)
print("ans_entropy = ", ans_entropy)

def I(p):
    # To avoid math domain error, add a small number
    return -p*math.log2(p+1e-10) - (1-p)*math.log2(1-p+1e-10)

def information_gain(data, mask):
    """
    This function will calculate the information gain
    args:
    * data(type: DataFrame): the data you're calculating for the information gain
    * mask(type: Series): partition information(left/right) of current input data,
    - boolean 1(True) represents split to left subtree
    - boolean 0(False) represents split to right subtree
    return:
    * ig(type: float): the information gain you can obtain by classify data with this given mask
    """
    entropy0 = entropy(data)
    l = data[Label].values
    num, num_left, num_right = len(mask), sum(mask), len(mask) - sum(mask)
    if num_right*num_left == 0: return 0 # To avoid division by zero

    left = 0; right = 0
    for i, j in zip(mask, l):
        if i and j: left += 1
        if not i and j: right += 1
    entropy1 = num_right/num * I(right/num_right) + num_left/num * I(left/num_left)

    ig = entropy0 - entropy1
    return ig

# [Note] You have to save the value of "ans_informationGain" into your output file
temp1 = np.zeros((int(input_data.shape[0]/4), 1), dtype=bool)
temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/4), 1)), dtype=bool)
temp_mask = np.concatenate((temp1, temp2))
df_mask = pd.DataFrame(temp_mask, columns=['mask'])
ans_informationGain = information_gain(input_data, df_mask['mask'])
print("ans_informationGain = ", ans_informationGain)

"""
## Step3 : Find the Best Split
Find the best split combination, **feature** and **threshold**, by calculating the information gain

"""

def find_best_split(data):
    """
    This function will find the best split combination of data
    args:
    * data(type: DataFrame): the input data
    return
    * best_ig(type: float): the best information gain you obtain
    * best_threshold(type: float): the value that splits data into 2 branches
    * best_feature(type: string): the feature that splits data into 2 branches
    """
    best_ig = -math.inf
    best_threshold = 0.0
    best_feature = ''
    l = data[Label].values
    dim = list(data.columns)[:-1]

    for col in dim:
        X = data[col].values
        #MIN, MAX = X.min(), X.max()
        delta = 0.05*X.std()
        threshold = X.mean() - 1.5*X.std()
        MAX = X.max()

        for t in range(60):
            mask = [1 if i <= threshold else 0 for i in X]
            ig = information_gain(data, mask)
            if ig > best_ig:
                best_ig = ig
                best_threshold = threshold
                best_feature = col
            threshold += delta

    return best_ig, best_threshold, best_feature

# [Note] You have to save the value of "ans_ig", "ans_value", and "ans_name" into the output file
ans_ig, ans_value, ans_name = find_best_split(input_data)
print("ans_ig = ", ans_ig)
print("ans_value = ", ans_value)
print("ans_name = ", ans_name)

"""
## Step4 : Split into 2 branches
Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree
"""

def make_partition(data, feature, threshold):
    """
    This function will split the data into 2 branches
    args:
    * data(type: DataFrame): the input data
    * feature(type: string): the attribute(column name)
    * threshold(type: float): the threshold for splitting the data
    return:
    * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold
    * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold
    """
    left = data.loc[(data[feature] <= threshold)]
    right = data.loc[(data[feature] > threshold)]
    return left, right


# [Note] You have to save the value of "ans_left" into the output file
left, right = make_partition(input_data, 'age', 61.0)
ans_left = left.shape[0]
print("ans_left = ", ans_left)

"""
## Step5 : Build Decision Tree
Use the above functions to implement the decision tree

Instructions:
1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples
2.  Use function *find_best_split()* to find the best split combination
3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)
4. Use function *make_partition()* to split the data into two parts
5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively

"""

def build_tree(data, max_depth, min_samples_split, depth):
    """
    This function will build the decision tree
    args:
    * data(type: DataFrame): the data you want to apply to the decision tree
    * max_depth: the maximum depth of a decision tree
    * min_samples_split: the minimum number of instances required to do partition
    * depth: the height of the current decision tree
    return:
    * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)
    """
    # check the condition of current depth and the remaining number of samples
    if depth < max_depth and data.shape[0] >= min_samples_split:
        # call find_best_split() to find the best combination
        ig, threshold, feature = find_best_split(data)
        # check the value of information gain is greater than 0 or not
        if  ig > 0:
            # update the depth
            depth += 1
            # call make_partition() to split the data into two parts
            left, right = make_partition(data, feature, threshold)

            # If there is no data split to the left tree OR no data split to the right tree
            # !!
            # if left.shape[0] < min_samples_split or right.shape[0] < min_samples_split:
            if left.shape[0] == 0 or right.shape[0] == 0:
                # return the label of the majority
                label = data[Label].value_counts().idxmax()
                return label
            else:
                question = "{} {} {}".format(feature, "<=", threshold)
                subtree = {question: []}

                # call function build_tree() to recursively build the left subtree and right subtree
                left_subtree = build_tree(left, max_depth, min_samples_split, depth)
                right_subtree = build_tree(right, max_depth, min_samples_split, depth)

                if left_subtree == right_subtree:
                    subtree = left_subtree
                else:
                    subtree[question].append(left_subtree)
                    subtree[question].append(right_subtree)
        else:
          # return the label of the majority
            label = data[Label].value_counts().idxmax()
            return label
    else:
        # return the label of the majority
        label = data[Label].value_counts().idxmax()
        #print(label)
        return label
    return subtree

"""
An example of the output from *build_tree()*
```
{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}
```
Therefore,
```
ans_features = ['bmi', 'age']
ans_thresholds = [33.5, 68.5]
```


"""

ans_features = []
ans_thresholds = []

decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)
decisionTree

def get_ans_key(dictionary):
    if isinstance(dictionary, dict):
        key = list(dictionary.keys())[0]
        s = key.split()
        ans_features.append(s[0])
        ans_thresholds.append(s[2])
        l, r = dictionary[key]
        get_ans_key(l)
        get_ans_key(r)
    return

get_ans_key(decisionTree)

# [Note] You have to save the features in the "decisionTree" structure (from root to branch and leaf) into the output file
ans_features

# [Note] You have to save the corresponding thresholds for the features in the "ans_features" list into the output file
ans_thresholds

"""
## Step6 : Save answers
"""

basic = []
basic.append(ans_entropy)
basic.append(ans_informationGain)
basic.append(ans_ig)
basic.append(ans_value)
basic.append(ans_name)
basic.append(ans_left)
for i in range(len(ans_features)):
    basic.append(ans_features[i])
for m in range(len(ans_thresholds)):
    basic.append(ans_thresholds[m])

"""
## Step7 : Split data
Split data into training set and validation set
> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data.
"""

num_train = 20
num_validation = 10

training_data = input_data.iloc[:num_train]
validation_data = input_data.iloc[-num_validation:]

y_train = training_data[["diabetes_mellitus"]]
x_train = training_data.drop(['diabetes_mellitus'], axis=1)
y_validation = validation_data[["diabetes_mellitus"]]
x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)
y_validation = y_validation.values.flatten()

print(input_data.shape)
print(training_data.shape)
print(validation_data.shape)

"""
## Step8 to Step 10 : Make predictions with a decision tree

Define the attributions of the decision tree
> You **cannot** modify the values of these attributes in this part
"""

max_depth = 2
depth = 0
min_samples_split = 2
n_features = x_train.shape[1]

"""
We have finished the function '*classify_data()*' below, however, you can modify this function if you prefer completing it on your own way.
"""

def classify_data(instance, tree):
    """
    This function will predict/classify the input instance
    args:
    * instance: a instance(case) to be predicted
    return:
    * answer: the prediction result (the classification result)
    """
    equation = list(tree.keys())[0]
    if equation.split()[1] == '<=':
        temp_feature = equation.split()[0]
        temp_threshold = equation.split()[2]
        if instance[temp_feature] > float(temp_threshold):
            answer = tree[equation][1]
        else:
            answer = tree[equation][0]
    else:
        if instance[equation.split()[0]] in (equation.split()[2]):
            answer = tree[equation][0]
        else:
            answer = tree[equation][1]

    if not isinstance(answer, dict):
        return answer
    else:
        return classify_data(instance, answer)


def make_prediction(tree, data):
    """
    This function will use your pre-trained decision tree to predict the labels of all instances in data
    args:
    * tree: the decision tree
    * data: the data to predict
    return:
    * y_prediction: the predictions
    """

    # [Note] You can call the function classify_data() to predict the label of each instance
    y_prediction = []
    for index, row in data.iterrows():
        y_prediction.append(classify_data(row, tree))

    return y_prediction


def calculate_score(y_true, y_pred):
    """
    This function will calculate the f1-score of the predictions
    args:
    * y_true: the ground truth
    * y_pred: the predictions
    return:
    * score: the f1-score
    """
    score = f1_score(y_true, y_pred)

    return score

decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)
print(decision_tree)

y_pred = make_prediction(decision_tree, x_validation)
print(y_validation, y_pred)

# [Note] You have to save the value of "ans_f1score" the your output file
ans_f1score = calculate_score(y_validation, y_pred)
print("ans_f1score = ", ans_f1score)

"""
## Step11 : Write the Output File
Save all of your answers in a csv file, named as **hw2_basic.csv**
"""

ans_path = 'hw2_basic.csv'

# [Note] You have to save the value of "ans_f1score" into the output file
basic.append(ans_f1score)
print(basic)

pd.DataFrame(basic).to_csv(ans_path, header = None, index = None)

"""
# **Advanced Part** (35%)
"""

"""
## Step1: Load the input data
First, load the input file **hw2_input_advanced.csv**
"""

advanced_data = pd.read_csv('hw2_input_advanced.csv')

"""You can split *advanced_data* into training set and validaiton set"""

idx = int(0.8*len(advanced_data))
training_data = advanced_data[:idx]
validation_data = advanced_data[idx:]

"""
## Step2 : Load the test data
Load the input file **hw2_input_test.csv** to make predictions with the pre-trained random forest model
"""

x_test = pd.read_csv('hw2_input_test.csv')
x_test

"""
## Step3 : Build a Random Forest

Define the attributions of the random forest
> * You **can** modify the values of these attributes in advanced part
> * Each tree can have different attribute values
> * There must be **at least** 3 decision trees in the random forest model
> * Must use function *build_tree()* to build a random forest model
> * These are the parameters you can adjust :


    ```
    max_depth =
    depth = 0
    min_samples_split =
    
    # total number of trees in a random forest
    n_trees =

    # number of features to train a decision tree
    n_features =

    # the ratio to select the number of instances
    sample_size =
    n_samples = int(training_data.shape[0] * sample_size)
    ```
"""

# Define the attributes
max_depth = 7
depth = 0
min_samples_split = 10

# total number of trees in a random forest
n_trees = 31

# number of features to train a decision tree
n_features = 11

# the ratio to select the number of instances
sample_size = 0.5
min_shape = min(sum(training_data[Label]), len(training_data[Label]) - sum(training_data[Label]))
n_samples = int(min_shape * sample_size)

# any other settings
random.seed(0)
n_dimensions = 18

def feature_selection(data, dim, n_dimensions):
    label0 = data.loc[(data[Label] == 0)]
    label1 = data.loc[(data[Label] == 1)]
    n = data.shape[0]
    n0, n1 = len(label0), len(label1)
    fisher_score = dict()
    if n0 * n1 == 0: return dim

    for feature in dim:
        mean0 = label0[feature].mean()
        mean1 = label1[feature].mean()
        mean = data[feature].mean()
        sw = (sum((label0[feature] - mean0)**2) + sum((label1[feature] - mean1)**2)) / n
        sb = n0/n * ((mean0 - mean)**2) + n1/n * ((mean1 - mean)**2)
        if sw == 0:
            fisher_score[feature] = 0
        else:
            fisher_score[feature] = sb/sw

    #print(fisher_score)
    reduced_dim = sorted(dim, key = lambda k: fisher_score[k], reverse=True)
    return reduced_dim[:n_dimensions]

def build_forest(data, n_trees, n_features, n_samples):
    """
    This function will build a random forest.
    args:
    * data: all data that can be used to train a random forest
    * n_trees: total number of tree
    * n_features: number of features
    * n_samples: number of instances
    return:
    * forest: a random forest with 'n_trees' of decision tree
    """
    init_dim = list(data.columns)[:-1]
    Dim = feature_selection(data, init_dim, n_dimensions)
    #print(Dim)
    forest = []
    data0 = data.loc[(data[Label] == 0)]
    data1 = data.loc[(data[Label] == 1)]

    for i in range(n_trees):
        depth = 0
        features = random.sample(Dim, n_features)
        features.append(Label)
        df0 = data0[features].sample(n_samples)
        df1 = data1[features].sample(n_samples)
        df = pd.concat([df0, df1])
        # must reuse function build_tree()
        tree = build_tree(df, max_depth, min_samples_split, depth)
        forest.append(tree)

    return forest

forest = build_forest(training_data, n_trees, n_features, n_samples)

"""
## Step4 : Make predictions with the random forest
> Note: Please print the f1-score of the predictions of each decision tree
"""

y_trees = []

def make_prediction_forest(forest, data):
    """
    This function will use the pre-trained random forest to make the predictions
    args:
    * forest: the random forest
    * data: the data used to predict
    return:
    * y_prediction: the predicted results
    """
    for t in forest:
        y_trees.append(make_prediction(t, data))

    y_prediction = [int(sum(i) > len(i)/2) for i in zip(*y_trees)]

    return y_prediction

# for validation
x0 = validation_data.drop(columns = ['diabetes_mellitus'])
y0 = validation_data['diabetes_mellitus'].values
y1 = make_prediction_forest(forest, x0)

# F1-score
for i, y in enumerate(y_trees):
    print(i, "F1-Score=", calculate_score(y0, y))

print("\nF1-score=", calculate_score(y0, y1))

y_pred_test = make_prediction_forest(forest, x_test)

"""
## Step5 : Write the Output File
Save your predictions from the **random forest** in a csv file, named as **hw2_advanced.csv**
"""

advanced = []
for i in range(len(y_pred_test)):
    advanced.append(y_pred_test[i])

advanced_path = 'hw2_advanced.csv'
pd.DataFrame(advanced).to_csv(advanced_path, header = None, index = None)