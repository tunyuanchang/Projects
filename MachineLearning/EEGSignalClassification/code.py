# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

"""

from google.colab import drive
import os
drive.mount('/content/drive')

os.chdir('/content/drive/MyDrive/11110/ML/hw/HW5')

"""
# **HW5: Brain signal classification**

In *HW 5*, you need to finish:

1.  Model Implementation Part: Implement LSTM and EEGNet models to predict the label of each samples.

2.  Model Competition Part: Implementing a model to reach better accuracy performance.
"""

# Commented out IPython magic to ensure Python compatibility.
### for tracking execution time
!pip install ipython-autotime
# %load_ext autotime

import numpy as np
import os
import math
import csv
import matplotlib.pyplot as plt
# Import the packages you need here

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.constraints import max_norm

data = np.load('data.npz')
label = np.load('label.npz')
num_classes = 6

X_train = data['X_train']
X_val = data['X_val']
X_test = data['X_test']

Y_train = label['Y_train']
Y_val = label['Y_val']

X_train.shape, X_val.shape, X_test.shape

Y_train.shape, Y_val.shape

"""
## Model Implementation Part
"""

"""
### LSTM
"""

# Build your model here:
lstm_model = models.Sequential()
lstm_model.add(tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2],)))
lstm_model.add(layers.LSTM(units=10, return_sequences=True))
lstm_model.add(layers.Flatten())
lstm_model.add(layers.Dense(48, activation='relu'))
lstm_model.add(layers.Dense(num_classes, activation='softmax'))
lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=3e-3), metrics='sparse_categorical_accuracy')
lstm_history = lstm_model.fit(X_train, Y_train, batch_size=16, epochs=10, validation_data=(X_val, Y_val))
#lstm_model.summary()

plt.plot(lstm_history.history['loss'])
plt.plot(lstm_history.history['val_loss'])
plt.title('LSTM model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

output = lstm_model.predict(X_test)
output = np.argmax(output, axis = 1).reshape(output.shape[0], 1).astype('int')
assert(output.shape == (190, 1))
np.savetxt('lstm_output.csv', output, delimiter=",")
#print(output)

"""
### EEGNet
"""

# Build your model here:
eegnet = models.Sequential()
eegnet.add(tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1)))
#block1
eegnet.add(layers.Conv2D(32, (1, X_train.shape[2]), padding="same"))
eegnet.add(layers.BatchNormalization())
eegnet.add(layers.DepthwiseConv2D(X_train.shape[1], 1, depth_multiplier=2, depthwise_constraint=max_norm(1.)))
eegnet.add(layers.BatchNormalization())
eegnet.add(layers.Activation('elu'))
eegnet.add(layers.AveragePooling2D((1, 4)))
eegnet.add(layers.Dropout(0.25))
#block2
eegnet.add(layers.SeparableConv2D(96, (1, 16), padding='same'))
eegnet.add(layers.BatchNormalization())
eegnet.add(layers.Activation('elu'))
eegnet.add(layers.AveragePooling2D((1, 8)))
eegnet.add(layers.Dropout(0.25))

eegnet.add(layers.Flatten())
eegnet.add(layers.Dense(num_classes, activation='softmax', kernel_constraint=max_norm(0.25)))

eegnet.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=3e-3), metrics='sparse_categorical_accuracy')
eegnet_history = eegnet.fit(X_train, Y_train, batch_size=16, epochs=10, validation_data=(X_val, Y_val))
#eegnet.summary()

plt.plot(eegnet_history.history['loss'])
plt.plot(eegnet_history.history['val_loss'])
plt.title('EEGNet model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

output = eegnet.predict(X_test)
output = np.argmax(output, axis = 1).reshape(output.shape[0], 1).astype('int')
assert(output.shape == (190, 1))
np.savetxt('eegnet_output.csv', output, delimiter=",")
#print(output)

"""
## Model Competition Part
"""

# Build your model here:
model = eegnet

output = model.predict(X_test)
output = np.argmax(output, axis = 1).reshape(output.shape[0], 1).astype('int')
assert(output.shape == (190, 1))
np.savetxt('competition_output.csv', output, delimiter=",")
#print(output)

